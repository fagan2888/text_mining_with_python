{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text\n",
    "\n",
    "The most important step in text analysis is preprocessing. This involves preparing the raw text so that it is machine-readable. For example, one can separate an article into several paragraphs, and for each paragraph separate the sentences into a list of words. One can cut off the ending of words to keep only the 'stem' or 'root' word. There are several possibilities. In this notebook, we will walk through some of the main tools to preprocess text.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is one of the first steps in preprocessing. It involves splitting a document into tokens. Often, this is simply converting a sentence or paragraph (however we define a document) into a list of words. We can do this by applying the <b>split()</b> method to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Federal', 'Reserve', 'targeting', 'of', 'a', '2%', 'inflation', 'led', 'to', 'better', 'price', 'stability!!!']\n"
     ]
    }
   ],
   "source": [
    "sent = \"The Federal Reserve targeting of a 2% inflation led to better price stability!!!\"\n",
    "\n",
    "tokens = sent.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is relatively straightforward. One should consider hyphenated terms should be tokenized into multiple tokens. \n",
    "\n",
    "It is also useful, when thinking of tokenization, to understand how documents are to be identified. Do we want analyses to be done using individual sentences, paragraphs, etc. We will discuss more of this when we apply document-level analyses such as topic modelling or word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalizing text is the process of converting a text into a more uniform sequence.  \n",
    "\n",
    "### Stemming\n",
    "\n",
    "The first normalization technique we will go through is stemming a word, which is reducing a word to its root stem. For example, the words cook, cooked, cooking, cooks could be transformed to simply <b>cook</b>. There are two main ways of stemming a word: Porter Stemmer and Lancaster Stemming. Let's implement it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK module is the Natural Language Toolkit library developed by researchers at UPenn. It contains a whole suite of useful preprocessing modules, one of which is the PorterStemmer. We will apply to the sentence, <b>sent</b>, \"The Federal Reserve targeting of a 2% inflation led to better price stability!!!\". We split the sentence using the <b>split</b> function so that each element of the list is a unique token.\n",
    "\n",
    "First, we need to create a new Porter stemmer. Then, we will use a <a href=https://www.pythonforbeginners.com/basics/list-comprehensions-in-python>list comprehension</a> to apply the stemmer to every word in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cook\n",
      "cook\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example stem\n",
    "print(stemmer.stem(\"cooking\"))\n",
    "print(stemmer.stem(\"cooked\"))\n",
    "print(stemmer.stem(\"cooks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply it to each token using the list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List\n",
      "['The', 'Federal', 'Reserve', 'targeting', 'of', 'a', '2%', 'inflation', 'led', 'to', 'better', 'price', 'stability!!!']\n",
      "Stemmed List\n",
      "['the', 'feder', 'reserv', 'target', 'of', 'a', '2%', 'inflat', 'led', 'to', 'better', 'price', 'stability!!!']\n"
     ]
    }
   ],
   "source": [
    "tokens_stemmed = [stemmer.stem(el) for el in tokens]\n",
    "print(\"Original List\")\n",
    "print(tokens)\n",
    "print(\"Stemmed List\")\n",
    "print(tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "Lemmatizing is similar to stemming. The main difference is that lemmatizing will use a vocabulary and the morphology of a word to transform it to the inflected forms. For example, the word \"ran\" is just a different form of the word \"run\", so we could group these words together as just \"run\".  \n",
    "\n",
    "Similar to the Porter Stemmer, we will use a lemmatizer from the nltk.stem library. After initializing the lemmatizer, we can apply it to individual words. The WordNetLemmatizer().lemmatize function requires one other argument, the part-of-speech. This defaults to just the noun, however we could also put in an adjective (\"a\"), adverb (\"r\"), noun (\"n\") or verb (\"v\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      "ran\n",
      "Lemmatizing\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Stemming\")\n",
    "print(stemmer.stem(\"ran\"))\n",
    "print(\"Lemmatizing\")\n",
    "print(lemmatizer .lemmatize(\"ran\",pos=\"v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to lemmatize properly, we should identify whether a word pertains to a noun, adverb, verb or adjective. We can do this by running the part-of-speech tagging function (the <b>pos_tag</b> function imported with from nltk import pos_tag) on each token. This will return a tuple where the second element is the part of speech. Since the wordnet lemmatized needs to be an \"a\", \"v\", \"n\" or \"r\", and the pos_tag function uses the <a href=https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>Penn Treebank Part-of-Speech Classification</a>, we will write a short function which does this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('targeting', 'VBG'), ('led', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    # Return whether \"tag\" is adjective, \n",
    "    # noun, verb or adverb\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "print(nltk.pos_tag([\"targeting\",\"led\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a list comprehension on this list of tuples. The first element of the tuple is just the word, while the second element of the tuple is what we will pass to the get_wordnet_pos function to return either a \"a\", \"n\", \"r\" or \"v\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List\n",
      "['The', 'Federal', 'Reserve', 'targeting', 'of', 'a', '2%', 'inflation', 'led', 'to', 'better', 'price', 'stability!!!']\n",
      "Stemmed List\n",
      "['The', 'Federal', 'Reserve', 'target', 'of', 'a', '2%', 'inflation', 'lead', 'to', 'well', 'price', 'stability!!!']\n"
     ]
    }
   ],
   "source": [
    "# Get Part-of-Speeches\n",
    "tokens_pos = nltk.pos_tag(tokens)\n",
    "# Use list of POSs to lemmatize\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(el[0],get_wordnet_pos(el[1])) for  el in tokens_pos]\n",
    "print(\"Original List\")\n",
    "print(tokens)\n",
    "print(\"Stemmed List\")\n",
    "print(tokens_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case-sensitivity\n",
    "\n",
    "To help with normalization, it is also useful to lowercase sentences before we split them up into tokens. This is easily done with just the \"lower()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence\n",
      "The Federal Reserve targeting of a 2% inflation led to better price stability!!!\n",
      "Lower-case sentence\n",
      "the federal reserve targeting of a 2% inflation led to better price stability!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence\")\n",
    "print(sent)\n",
    "print(\"Lower-case sentence\")\n",
    "print(sent.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "\n",
    "Most analyses are only concerned with text itself and not so much punctuation or numbers. We can replace anything which is not a letter using REgex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence\n",
      "The Federal Reserve targeting of a 2% inflation led to better price stability!!!\n",
      "Remove Punctuation/Numbers Sentence\n",
      "The Federal Reserve targeting of a    inflation led to better price stability   \n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "print(\"Original Sentence\")\n",
    "print(sent)\n",
    "print(\"Remove Punctuation/Numbers Sentence\")\n",
    "print(re.sub(\"[^A-Za-z]\",\" \",sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REGex pattern \"[A-Za-z]\" identifies anything which is an upper or lower case letter. By placing the \"^\" in front, we will match anything which is NOT a letter. Hence, the command <b>re.sub(\"[^A-Za-z\",\" \",sent)) </b> replaces anything in <b>sent</b> which is NOT a letter with an empty space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canonicalization\n",
    "\n",
    "Canonicalization is the process of converting any phrase which has more than one possible representation with a generic form. For example, the Federal Reserve may be referred to as the Fed, the Board, the Fed Board, Fed Reserve. We can replace any instance of a predefined list of names for the Federal Reserve with a unifying representation, such as <b>Federal_Reserve</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Federal_Reserve targeting of a 2% inflation led to better price stability!!!\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(\"Fed Board|Federal Reserve|the Board\",\"Federal_Reserve\",sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "\n",
    "The last step we will learn for preprocessing is noise removal. \n",
    "\n",
    "One of the most common ways to remove noise is to remove stop words from your text. These are words which are highly common across documents, such as the words \"the\", \"a\", \"and\", \"yours\", etc. These are words which encompass a large percentage of text, but provide little information.\n",
    "\n",
    "A common approach to removing stopwords is via the nltk library. We will import the stop words in english, save it as a variable <b>sw</b>, and check to see if each word in tokens (which is a list of words) is a stop word. We will save a new variable which is a list of words which are not stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Federal', 'Reserve', 'targeting', 'of', 'a', '2%', 'inflation', 'led', 'to', 'better', 'price', 'stability!!!']\n",
      "['The', 'Federal', 'Reserve', 'targeting', '2%', 'inflation', 'led', 'better', 'price', 'stability!!!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "tokens_excl_sw = [word for word in tokens if word not in sw]\n",
    "print(tokens)\n",
    "print(tokens_excl_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Noise\n",
    "\n",
    "Headers and footers, boilerplate text, or even reduntant HTML/XML are all examples of noise which we should be cognizant of when preprocessing documents. This type of text should be removed. We can find any sentence/paragraph/phrase which contains any of these noisy texts and replace it with an empty white space, for example. The best approach to identify noisy text is typically REgex. This step of preprocessing requires heavy inspection of the individual documents on the researchers part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert a set of sentences into a document term matrix, we will use the functions in the module <b>sklearn.feature_extraction.text</b>. \n",
    "\n",
    "For example, suppose we have the sentences:\n",
    "    \n",
    "    1. this is the first sentence\n",
    "    2. here is the second sentence\n",
    "    3. that would make this sentence the third sentence\n",
    "    \n",
    "We can summarize all of this information in a document-term matrix. The rows of this matrix will be each sentence (for example: <i>this is the first sentence</i> will be its own row), and the columns will be the vocabulary (the unique words which compose the corpus. That is, 'first', 'here', 'is', 'make', 'one', 'second', 'sentence', 'that', 'the', 'third', 'this', 'would').\n",
    "\n",
    "Let's see what this looks like in action. First we will need to import the <b>CountVectorizer</b> from the module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the sentences in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"this is the first sentence\",\n",
    "              \"here is the second sentence\",\n",
    "              \"that would make this sentence the third sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will generate a CountVectorizer instance. Think of this variable as a \"machine\". It will start off with no knowledge of any sentence, and then we will feed in the sentences we created above into a variable <b>X</b>. That resulting <b>X</b> is the object which contains the document-term matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 1 0 1 0 1 0]\n",
      " [0 1 1 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 1 0 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a sentence. The ones mean that the word which pertains to that column show up in that row's sentence. But what are the column names? We can get that from the vectorizer we initiated above with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'here', 'is', 'make', 'second', 'sentence', 'that', 'the', 'third', 'this', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be easier to save all this information in a Pandas dataframe, so that we can easily visualize this matrix. We will use Pandas since this module is great for saving datasets with column names. When we initiate a dataframe in Pandas, we should input the column names (through the <b>columns</b> argument) as well as the actual data (through the appropriately named <b>data</b> argument). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>here</th>\n",
       "      <th>is</th>\n",
       "      <th>make</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first  here  is  make  second  sentence  that  the  third  this  would\n",
       "0      1     0   1     0       0         1     0    1      0     1      0\n",
       "1      0     1   1     0       1         1     0    1      0     0      0\n",
       "2      0     0   0     1       0         2     1    1      1     1      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentences_df = pd.DataFrame(columns=vectorizer.get_feature_names(),\n",
    "                           data = X.toarray())\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. The first row pertains to the sentence <i>this is the first sentence</i>, which contains the words \"first\", \"is\", \"sentence\",\"the\", and \"this\". \n",
    "\n",
    "Notice that CountVectorizer gives you the <i>count</i>/frequency of each word. Notice how in the third sentence the word \"sentence\" shows up twice and it correctly has a value of 2 as a result of this. \n",
    "\n",
    "## TF-IDF Weights\n",
    "\n",
    "What if we wanted to weigh each value depending on how frequent the word is in the document and across documents. For example, the word \"the\" shows up in all 3 documents so that word does not provide much information to discriminate across sentences. \n",
    "\n",
    "This is quite easily accomplished in Python through the <b>TfidfVectorizer</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>here</th>\n",
       "      <th>is</th>\n",
       "      <th>make</th>\n",
       "      <th>second</th>\n",
       "      <th>sentence</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.420183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.55249</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397699</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.469775</td>\n",
       "      <td>0.397699</td>\n",
       "      <td>0.234887</td>\n",
       "      <td>0.397699</td>\n",
       "      <td>0.302460</td>\n",
       "      <td>0.397699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      first     here        is      make   second  sentence      that  \\\n",
       "0  0.591887  0.00000  0.450145  0.000000  0.00000  0.349578  0.000000   \n",
       "1  0.000000  0.55249  0.420183  0.000000  0.55249  0.326310  0.000000   \n",
       "2  0.000000  0.00000  0.000000  0.397699  0.00000  0.469775  0.397699   \n",
       "\n",
       "        the     third      this     would  \n",
       "0  0.349578  0.000000  0.450145  0.000000  \n",
       "1  0.326310  0.000000  0.000000  0.000000  \n",
       "2  0.234887  0.397699  0.302460  0.397699  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "                \n",
    "vectorizer2 = TfidfVectorizer()\n",
    "Y = vectorizer2.fit_transform(sentences)\n",
    "\n",
    "sentences_df_tfidf = pd.DataFrame(columns=vectorizer2.get_feature_names(),\n",
    "                           data = Y.toarray())\n",
    "sentences_df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the best way of interpreting this? The word \"the\" has a relatively low TF-IDF score in the 3rd document. Why is that? Well its for 2 reasons: 1) The word \"the\" shows up in all 3 documents, so compared to other words in the same sentence, it has a pretty low score 2) It has an especially low score if the document has many words in that sentence. Take document 3. There are 7 words in that sentence and \"the\" only shows up once. Hence it will have a lower score.\n",
    "\n",
    "## Main Takeaways\n",
    "This sums up the general ideas for preprocessing. What you should walk away from this notebook are:\n",
    "\n",
    "1. How do you Normalize a set of documents?\n",
    "2. How do you Tokenize each document?\n",
    "3. How do you remove noise from each document?\n",
    "4. How can you summarize the documents in a document-term matrix? \n",
    "    \n",
    "    4a. Using Counts\n",
    "    \n",
    "    4b. Using TFIDF weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
